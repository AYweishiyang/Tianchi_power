{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdata(data_path, user_id):\n",
    "    power = pd.read_csv(data_path)\n",
    "    power = power.loc[(power.user_id == user_id), :]\n",
    "    data = power.drop([\"user_id\"], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting_data(data):\n",
    "    # Separate the data into features and targets\n",
    "    target_fields = ['power_consumption']\n",
    "    features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_predict(feature, pre):\n",
    "    feature = feature.append(pre, ignore_index=True)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_variables(data):\n",
    "    dummy_fields = ['year', 'month', 'day', 'weekday', 'season', 'climate', 'windspeed']\n",
    "\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(data[each], prefix=each, drop_first=False)\n",
    "        data = pd.concat([data, dummies], axis=1)\n",
    "\n",
    "    fields_to_drop = ['year', 'month', 'day', 'weekday', 'season', 'climate', 'windspeed']\n",
    "    data = data.drop(fields_to_drop, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaling_variable(data, scaled_features, quant_features):\n",
    "    # Store scalings in a dictionary so we can convert back later\n",
    "    for each in quant_features:\n",
    "        max_, min_ = data[each].max(), data[each].min()\n",
    "        scaled_features[each] = [max_, min_]\n",
    "        data.loc[:, each] = (data[each] - min_)/(max_ - min_)\n",
    "    return data, scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting_validation_test(feature, target):\n",
    "    # splitting test set\n",
    "    train_features, test_features, train_targets = features[:-30], features[-30:], targets\n",
    "    \n",
    "    #splitting validation set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return train_features, train_targets, val_features, val_targets, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saving_answer(predicton, user_id):\n",
    "    answer = pd.read_csv(\"prediction/answer.csv\")\n",
    "    answer[\"id\"+str(user_id)] = predicton.reshape(len(predicton)).tolist()\n",
    "    answer.to_csv(\"prediction/answer.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.2, name='leaky_relu'):\n",
    "    return tf.maximum(x, alpha * x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(features,targets, batch_size):\n",
    "    batch_number = len(features)//batch_size\n",
    "    for i in range(batch_number+1):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        if i == batch_number:\n",
    "            yield features[start:], targets[start:]\n",
    "        else:\n",
    "            yield features[start:end], targets[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_variable():\n",
    "    input_ = tf.placeholder(tf.float32, [None, 65], name = \"inputs\")  # input\n",
    "    label_ = tf.placeholder(tf.float32, [None, 1], name = \"outputs\")  # output \n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")        # probability to keep units\n",
    "    lr = tf.placeholder(tf.float32, name = \"learning_rate\")           # learning rate\n",
    "    \n",
    "    return input_, label_, keep_prob, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(ac_fn, keep_prob, input_, label_):\n",
    "    layer1 = tf.layers.dense(input_, 128, activation = ac_fn, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    dropout1 = tf.nn.dropout(layer1, keep_prob)\n",
    "    \n",
    "    layer2 = tf.layers.dense(dropout1, 256, activation = ac_fn, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    dropout2 = tf.nn.dropout(layer2, keep_prob)\n",
    "\n",
    "    logits = tf.layers.dense(dropout2, 1, activation = tf.abs, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    cost = tf.losses.mean_squared_error(logits, label_)\n",
    "    \n",
    "    return logits, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_opt(cost, learning_rate, beta1 = 0.5):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1).minimize(cost)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, train_features, train_targets, \n",
    "                         val_features, val_targets, test_features, ac_fn):\n",
    "\n",
    "    input_, label_, keep_prob, lr = Create_variable()\n",
    "    logits, cost = model_loss(ac_fn, keep_prob, input_, label_)\n",
    "    optimizer = model_opt(cost, learning_rate)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            for x, y in get_batches(train_features.values,train_targets.values, batch_size):\n",
    "                feed = {input_: x, label_: y,  keep_prob: dropout, lr: learning_rate}\n",
    "                train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "\n",
    "            \n",
    "            feed={input_: val_features.values, label_: val_targets.values, keep_prob: 1}        \n",
    "            val_loss = sess.run(cost, feed_dict=feed)\n",
    "            \n",
    "            \n",
    "            #if epoch_i % 100 == 0:\n",
    "        print('Epoch {:>3}/{}   train_loss = {:.5f}   validation_loss = {:.5f}'.format(\n",
    "                    epoch_i,\n",
    "                    epoch_count,\n",
    "                    train_loss,\n",
    "                    val_loss))\n",
    "            \n",
    "            #losses['train'].append(train_loss)\n",
    "            #losses['validation'].append(val_loss)\n",
    "            \n",
    "        # validation result\n",
    "        feed={input_: val_features.values,  keep_prob: 1}\n",
    "        validation = sess.run(logits, feed_dict=feed)\n",
    "        \n",
    "        # prediction result\n",
    "        feed={input_: test_features.values,  keep_prob: 1}\n",
    "        prediction = sess.run(logits, feed_dict=feed)\n",
    "        \n",
    "        return losses, validation, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epoch_count = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(losses, user_id):\n",
    "    print(\"losses for user_id \" + str(user_id))\n",
    "    \n",
    "    plt.plot(losses['train'], label='Training loss')\n",
    "    plt.plot(losses['validation'], label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(ymax=0.02, ymin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_validation(validation, val_targets, scaled_features, user_id):\n",
    "    print(\"validation test for user_id \" + str(user_id))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    max_, min_ = scaled_features['power_consumption']\n",
    "    pre = validation*(max_ - min_) + min_\n",
    "    ax.plot(pre, label='Prediction')\n",
    "    ax.scatter(range(len(val_targets)), pre)\n",
    "    val = (val_targets*(max_ - min_) + min_).values\n",
    "    ax.plot(val, label='Data')\n",
    "    ax.scatter(range(len(val_targets)), val)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the prediction set\n",
    "add_data = pd.read_csv(\"prediction/predict.csv\")\n",
    "data_path = \"EN_Tianchi_power_v2.csv\"    \n",
    "user_id = 300\n",
    "data = readdata(data_path, user_id)    \n",
    "# split the dataset into features and targets\n",
    "features, targets = splitting_data(data)\n",
    "    \n",
    "# add the text set to the feature set\n",
    "features = add_predict(features, add_data)\n",
    "    \n",
    "# dummy the categorical variables\n",
    "features = dummy_variables(features)\n",
    "    \n",
    "# Use a dictionary to save the scaling value\n",
    "scaled_features = {}\n",
    "    \n",
    "# Using max-min scaleding for features\n",
    "quant_features = [\"temp\"]\n",
    "features, scaled_features = scaling_variable(features, scaled_features, quant_features)\n",
    "    \n",
    "# Using max-min scaleding for targets\n",
    "quant_features = [\"power_consumption\"]\n",
    "targets, scaled_features = scaling_variable(targets, scaled_features, quant_features)\n",
    "    \n",
    "# spliting the test set, validation set and training set\n",
    "train_features, train_targets, val_features, val_targets, test_features = splitting_validation_test(features, targets)\n",
    "    \n",
    "# Use a dictionary to save the loss so that we can show as a figure to analysis\n",
    "losses = {'train':[], 'validation':[]}\n",
    "    \n",
    "print(\"User_id \" + str(user_id) + \" training:\")\n",
    "# Training the model for each company\n",
    "losses, validation, prediction = build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, \n",
    "                                                    train_features, train_targets, val_features, val_targets, test_features, leaky_relu)\n",
    "# plot the loss\n",
    "plot_loss(losses, user_id)\n",
    "\n",
    "    # plot the validation test\n",
    "draw_validation(validation, val_targets, scaled_features, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read the prediction set\n",
    "add_data = pd.read_csv(\"prediction/predict.csv\")\n",
    "data_path = \"EN_Tianchi_power_v2.csv\"    \n",
    "user_id = 259\n",
    "data = readdata(data_path, user_id)    \n",
    "# split the dataset into features and targets\n",
    "features, targets = splitting_data(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_id 1300 training:\n",
      "Epoch 999/1000   train_loss = 0.00328   validation_loss = 0.01457\n",
      "User_id 1301 training:\n",
      "Epoch 999/1000   train_loss = 0.00388   validation_loss = 0.02340\n",
      "User_id 1302 training:\n",
      "Epoch 999/1000   train_loss = 0.00628   validation_loss = 0.01797\n",
      "User_id 1303 training:\n",
      "Epoch 999/1000   train_loss = 0.01189   validation_loss = 0.12375\n",
      "User_id 1304 training:\n",
      "Epoch 999/1000   train_loss = 0.00157   validation_loss = 0.01328\n",
      "User_id 1305 training:\n",
      "Epoch 999/1000   train_loss = 0.00335   validation_loss = 0.00432\n",
      "User_id 1306 training:\n",
      "Epoch 999/1000   train_loss = 0.00116   validation_loss = 0.00459\n",
      "User_id 1307 training:\n",
      "Epoch 999/1000   train_loss = 0.01026   validation_loss = 0.05053\n",
      "User_id 1308 training:\n",
      "Epoch 999/1000   train_loss = 0.00236   validation_loss = 0.02064\n",
      "User_id 1309 training:\n",
      "Epoch 999/1000   train_loss = 0.00375   validation_loss = 0.01918\n",
      "User_id 1310 training:\n",
      "Epoch 999/1000   train_loss = 0.01019   validation_loss = 0.04750\n",
      "User_id 1311 training:\n",
      "Epoch 999/1000   train_loss = 0.00215   validation_loss = 0.00923\n",
      "User_id 1312 training:\n",
      "Epoch 999/1000   train_loss = 0.00572   validation_loss = 0.03372\n",
      "User_id 1313 training:\n",
      "Epoch 999/1000   train_loss = 0.00687   validation_loss = 0.02568\n",
      "User_id 1314 training:\n",
      "Epoch 999/1000   train_loss = 0.00424   validation_loss = 0.02088\n",
      "User_id 1315 training:\n",
      "Epoch 999/1000   train_loss = 0.00187   validation_loss = 0.00930\n",
      "User_id 1316 training:\n",
      "Epoch 999/1000   train_loss = 0.00247   validation_loss = 0.03233\n",
      "User_id 1317 training:\n",
      "Epoch 999/1000   train_loss = 0.00293   validation_loss = 0.01279\n",
      "User_id 1318 training:\n",
      "Epoch 999/1000   train_loss = 0.00401   validation_loss = 0.04046\n",
      "User_id 1319 training:\n",
      "Epoch 999/1000   train_loss = 0.00568   validation_loss = 0.03310\n",
      "User_id 1320 training:\n",
      "Epoch 999/1000   train_loss = 0.00098   validation_loss = 0.00816\n",
      "User_id 1321 training:\n",
      "Epoch 999/1000   train_loss = 0.00261   validation_loss = 0.01067\n",
      "User_id 1322 training:\n",
      "Epoch 999/1000   train_loss = 0.00341   validation_loss = 0.00863\n",
      "User_id 1323 training:\n",
      "Epoch 999/1000   train_loss = 0.00776   validation_loss = 0.02632\n",
      "User_id 1324 training:\n",
      "Epoch 999/1000   train_loss = 0.00136   validation_loss = 0.00990\n",
      "User_id 1325 training:\n",
      "Epoch 999/1000   train_loss = 0.00441   validation_loss = 0.03089\n",
      "User_id 1326 training:\n",
      "Epoch 999/1000   train_loss = 0.00658   validation_loss = 0.01912\n",
      "User_id 1327 training:\n",
      "Epoch 999/1000   train_loss = 0.00446   validation_loss = 0.02001\n",
      "User_id 1328 training:\n",
      "Epoch 999/1000   train_loss = 0.00494   validation_loss = 0.02715\n",
      "User_id 1329 training:\n",
      "Epoch 999/1000   train_loss = 0.01014   validation_loss = 0.05255\n",
      "User_id 1330 training:\n",
      "Epoch 999/1000   train_loss = 0.00436   validation_loss = 0.02669\n",
      "User_id 1331 training:\n",
      "Epoch 999/1000   train_loss = 0.00664   validation_loss = 0.03514\n",
      "User_id 1332 training:\n",
      "Epoch 999/1000   train_loss = 0.00106   validation_loss = 0.01208\n",
      "User_id 1333 training:\n",
      "Epoch 999/1000   train_loss = 0.00502   validation_loss = 0.01509\n",
      "User_id 1334 training:\n",
      "Epoch 999/1000   train_loss = 0.00539   validation_loss = 0.04875\n",
      "User_id 1335 training:\n",
      "Epoch 999/1000   train_loss = 0.00632   validation_loss = 0.03147\n",
      "User_id 1336 training:\n",
      "Epoch 999/1000   train_loss = 0.00942   validation_loss = 0.03062\n",
      "User_id 1337 training:\n",
      "Epoch 999/1000   train_loss = 0.00148   validation_loss = 0.00236\n",
      "User_id 1338 training:\n",
      "Epoch 999/1000   train_loss = 0.00311   validation_loss = 0.02041\n",
      "User_id 1339 training:\n",
      "Epoch 999/1000   train_loss = 0.00253   validation_loss = 0.01313\n",
      "User_id 1340 training:\n",
      "Epoch 999/1000   train_loss = 0.00456   validation_loss = 0.01845\n",
      "User_id 1341 training:\n",
      "Epoch 999/1000   train_loss = 0.01594   validation_loss = 0.05617\n",
      "User_id 1342 training:\n",
      "Epoch 999/1000   train_loss = 0.00270   validation_loss = 0.01732\n",
      "User_id 1343 training:\n",
      "Epoch 999/1000   train_loss = 0.00399   validation_loss = 0.01588\n",
      "User_id 1344 training:\n",
      "Epoch 999/1000   train_loss = 0.01622   validation_loss = 0.06238\n",
      "User_id 1345 training:\n",
      "Epoch 999/1000   train_loss = 0.00474   validation_loss = 0.02731\n",
      "User_id 1346 training:\n",
      "Epoch 999/1000   train_loss = 0.00525   validation_loss = 0.01728\n",
      "User_id 1347 training:\n",
      "Epoch 999/1000   train_loss = 0.00541   validation_loss = 0.05636\n",
      "User_id 1348 training:\n",
      "Epoch 999/1000   train_loss = 0.00373   validation_loss = 0.01672\n",
      "User_id 1349 training:\n",
      "Epoch 999/1000   train_loss = 0.00956   validation_loss = 0.01062\n",
      "User_id 1350 training:\n",
      "Epoch 999/1000   train_loss = 0.00756   validation_loss = 0.06556\n",
      "User_id 1351 training:\n",
      "Epoch 999/1000   train_loss = 0.00382   validation_loss = 0.01867\n",
      "User_id 1352 training:\n",
      "Epoch 999/1000   train_loss = 0.00424   validation_loss = 0.01089\n",
      "User_id 1353 training:\n",
      "Epoch 999/1000   train_loss = 0.00177   validation_loss = 0.00262\n",
      "User_id 1354 training:\n",
      "Epoch 999/1000   train_loss = 0.00850   validation_loss = 0.03548\n",
      "User_id 1355 training:\n",
      "Epoch 999/1000   train_loss = 0.00460   validation_loss = 0.02214\n",
      "User_id 1356 training:\n",
      "Epoch 999/1000   train_loss = 0.00321   validation_loss = 0.01840\n",
      "User_id 1357 training:\n",
      "Epoch 999/1000   train_loss = 0.00192   validation_loss = 0.02093\n",
      "User_id 1358 training:\n",
      "Epoch 999/1000   train_loss = 0.00525   validation_loss = 0.02292\n",
      "User_id 1359 training:\n",
      "Epoch 999/1000   train_loss = 0.00798   validation_loss = 0.06289\n",
      "User_id 1360 training:\n",
      "Epoch 999/1000   train_loss = 0.00342   validation_loss = 0.01143\n",
      "User_id 1361 training:\n",
      "Epoch 999/1000   train_loss = 0.00524   validation_loss = 0.02209\n",
      "User_id 1362 training:\n",
      "Epoch 999/1000   train_loss = 0.00450   validation_loss = 0.01741\n",
      "User_id 1363 training:\n",
      "Epoch 999/1000   train_loss = 0.00584   validation_loss = 0.03076\n",
      "User_id 1364 training:\n",
      "Epoch 999/1000   train_loss = 0.00542   validation_loss = 0.03134\n",
      "User_id 1365 training:\n",
      "Epoch 999/1000   train_loss = 0.00604   validation_loss = 0.02858\n",
      "User_id 1366 training:\n",
      "Epoch 999/1000   train_loss = 0.01276   validation_loss = 0.00539\n",
      "User_id 1367 training:\n",
      "Epoch 999/1000   train_loss = 0.00645   validation_loss = 0.02759\n",
      "User_id 1368 training:\n",
      "Epoch 999/1000   train_loss = 0.00087   validation_loss = 0.00667\n",
      "User_id 1369 training:\n",
      "Epoch 999/1000   train_loss = 0.00339   validation_loss = 0.01575\n",
      "User_id 1370 training:\n",
      "Epoch 999/1000   train_loss = 0.00384   validation_loss = 0.01616\n",
      "User_id 1371 training:\n",
      "Epoch 999/1000   train_loss = 0.01001   validation_loss = 0.03905\n",
      "User_id 1372 training:\n",
      "Epoch 999/1000   train_loss = 0.00352   validation_loss = 0.03525\n",
      "User_id 1373 training:\n",
      "Epoch 999/1000   train_loss = 0.01358   validation_loss = 0.01904\n",
      "User_id 1374 training:\n",
      "Epoch 999/1000   train_loss = 0.00455   validation_loss = 0.02822\n",
      "User_id 1375 training:\n",
      "Epoch 999/1000   train_loss = 0.00591   validation_loss = 0.04074\n",
      "User_id 1376 training:\n",
      "Epoch 999/1000   train_loss = 0.00543   validation_loss = 0.00704\n",
      "User_id 1377 training:\n",
      "Epoch 999/1000   train_loss = 0.01158   validation_loss = 0.06284\n",
      "User_id 1378 training:\n",
      "Epoch 999/1000   train_loss = 0.00085   validation_loss = 0.00270\n",
      "User_id 1379 training:\n",
      "Epoch 999/1000   train_loss = 0.00428   validation_loss = 0.02124\n",
      "User_id 1380 training:\n",
      "Epoch 999/1000   train_loss = 0.00351   validation_loss = 0.01454\n",
      "User_id 1381 training:\n",
      "Epoch 999/1000   train_loss = 0.00668   validation_loss = 0.03173\n",
      "User_id 1382 training:\n",
      "Epoch 999/1000   train_loss = 0.01263   validation_loss = 0.02348\n",
      "User_id 1383 training:\n",
      "Epoch 999/1000   train_loss = 0.00316   validation_loss = 0.02075\n",
      "User_id 1384 training:\n",
      "Epoch 999/1000   train_loss = 0.00167   validation_loss = 0.00578\n",
      "User_id 1385 training:\n",
      "Epoch 999/1000   train_loss = 0.00374   validation_loss = 0.04014\n",
      "User_id 1386 training:\n",
      "Epoch 999/1000   train_loss = 0.00227   validation_loss = 0.01367\n",
      "User_id 1387 training:\n",
      "Epoch 999/1000   train_loss = 0.00011   validation_loss = 0.00092\n",
      "User_id 1388 training:\n",
      "Epoch 999/1000   train_loss = 0.00503   validation_loss = 0.03819\n",
      "User_id 1389 training:\n",
      "Epoch 999/1000   train_loss = 0.00258   validation_loss = 0.00736\n",
      "User_id 1390 training:\n",
      "Epoch 999/1000   train_loss = 0.00339   validation_loss = 0.01388\n",
      "User_id 1391 training:\n",
      "Epoch 999/1000   train_loss = 0.00384   validation_loss = 0.02232\n",
      "User_id 1392 training:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999/1000   train_loss = 0.00561   validation_loss = 0.02523\n",
      "User_id 1393 training:\n",
      "Epoch 999/1000   train_loss = 0.00193   validation_loss = 0.03424\n",
      "User_id 1394 training:\n",
      "Epoch 999/1000   train_loss = 0.00458   validation_loss = 0.01986\n",
      "User_id 1395 training:\n",
      "Epoch 999/1000   train_loss = 0.00194   validation_loss = 0.00458\n",
      "User_id 1396 training:\n",
      "Epoch 999/1000   train_loss = 0.00182   validation_loss = 0.00908\n",
      "User_id 1397 training:\n",
      "Epoch 999/1000   train_loss = 0.00518   validation_loss = 0.03629\n",
      "User_id 1398 training:\n",
      "Epoch 999/1000   train_loss = 0.01088   validation_loss = 0.04048\n",
      "User_id 1399 training:\n",
      "Epoch 999/1000   train_loss = 0.00261   validation_loss = 0.02413\n",
      "User_id 1400 training:\n",
      "Epoch 999/1000   train_loss = 0.00261   validation_loss = 0.01229\n",
      "User_id 1401 training:\n",
      "Epoch 999/1000   train_loss = 0.00384   validation_loss = 0.01698\n",
      "User_id 1402 training:\n",
      "Epoch 999/1000   train_loss = 0.00239   validation_loss = 0.01518\n",
      "User_id 1403 training:\n",
      "Epoch 999/1000   train_loss = 0.00458   validation_loss = 0.01465\n",
      "User_id 1404 training:\n",
      "Epoch 999/1000   train_loss = 0.00337   validation_loss = 0.02912\n",
      "User_id 1405 training:\n",
      "Epoch 999/1000   train_loss = 0.00622   validation_loss = 0.02877\n",
      "User_id 1406 training:\n",
      "Epoch 999/1000   train_loss = 0.00150   validation_loss = 0.00931\n",
      "User_id 1407 training:\n",
      "Epoch 999/1000   train_loss = 0.01023   validation_loss = 0.02465\n",
      "User_id 1408 training:\n",
      "Epoch 999/1000   train_loss = 0.00531   validation_loss = 0.01403\n",
      "User_id 1409 training:\n",
      "Epoch 999/1000   train_loss = 0.00446   validation_loss = 0.03218\n",
      "User_id 1410 training:\n",
      "Epoch 999/1000   train_loss = 0.00325   validation_loss = 0.01198\n",
      "User_id 1411 training:\n",
      "Epoch 999/1000   train_loss = 0.00320   validation_loss = 0.01732\n",
      "User_id 1412 training:\n",
      "Epoch 999/1000   train_loss = 0.00667   validation_loss = 0.02703\n",
      "User_id 1413 training:\n",
      "Epoch 999/1000   train_loss = nan   validation_loss = nan\n",
      "User_id 1414 training:\n",
      "Epoch 999/1000   train_loss = 0.00370   validation_loss = 0.01409\n",
      "User_id 1415 training:\n",
      "Epoch 999/1000   train_loss = 0.00471   validation_loss = 0.03004\n",
      "User_id 1416 training:\n",
      "Epoch 999/1000   train_loss = 0.00251   validation_loss = 0.01802\n",
      "User_id 1417 training:\n",
      "Epoch 999/1000   train_loss = 0.00188   validation_loss = 0.01182\n",
      "User_id 1418 training:\n",
      "Epoch 999/1000   train_loss = 0.00353   validation_loss = 0.02138\n",
      "User_id 1419 training:\n",
      "Epoch 999/1000   train_loss = 0.00760   validation_loss = 0.01694\n",
      "User_id 1420 training:\n",
      "Epoch 999/1000   train_loss = 0.00083   validation_loss = 0.00596\n",
      "User_id 1421 training:\n",
      "Epoch 999/1000   train_loss = 0.00484   validation_loss = 0.03638\n",
      "User_id 1422 training:\n",
      "Epoch 999/1000   train_loss = 0.00711   validation_loss = 0.02910\n",
      "User_id 1423 training:\n",
      "Epoch 999/1000   train_loss = 0.00005   validation_loss = 0.00133\n",
      "User_id 1424 training:\n",
      "Epoch 999/1000   train_loss = 0.00171   validation_loss = 0.02007\n",
      "User_id 1425 training:\n",
      "Epoch 999/1000   train_loss = 0.00445   validation_loss = 0.02590\n",
      "User_id 1426 training:\n",
      "Epoch 999/1000   train_loss = 0.00125   validation_loss = 0.02992\n",
      "User_id 1427 training:\n",
      "Epoch 999/1000   train_loss = 0.00479   validation_loss = 0.03752\n",
      "User_id 1428 training:\n",
      "Epoch 999/1000   train_loss = 0.00851   validation_loss = 0.05511\n",
      "User_id 1429 training:\n",
      "Epoch 999/1000   train_loss = 0.00540   validation_loss = 0.03495\n",
      "User_id 1430 training:\n",
      "Epoch 999/1000   train_loss = 0.00347   validation_loss = 0.01287\n",
      "User_id 1431 training:\n",
      "Epoch 999/1000   train_loss = 0.00421   validation_loss = 0.02377\n",
      "User_id 1432 training:\n",
      "Epoch 999/1000   train_loss = 0.00290   validation_loss = 0.02919\n",
      "User_id 1433 training:\n",
      "Epoch 999/1000   train_loss = 0.00500   validation_loss = 0.02527\n",
      "User_id 1434 training:\n",
      "Epoch 999/1000   train_loss = 0.00476   validation_loss = 0.02185\n",
      "User_id 1435 training:\n",
      "Epoch 999/1000   train_loss = 0.00189   validation_loss = 0.01751\n",
      "User_id 1436 training:\n",
      "Epoch 999/1000   train_loss = 0.00340   validation_loss = 0.02265\n",
      "User_id 1437 training:\n",
      "Epoch 999/1000   train_loss = 0.00261   validation_loss = 0.02136\n",
      "User_id 1438 training:\n",
      "Epoch 999/1000   train_loss = 0.00132   validation_loss = 0.00940\n",
      "User_id 1439 training:\n",
      "Epoch 999/1000   train_loss = 0.00413   validation_loss = 0.01638\n",
      "User_id 1440 training:\n",
      "Epoch 999/1000   train_loss = 0.00110   validation_loss = 0.01021\n",
      "User_id 1441 training:\n",
      "Epoch 999/1000   train_loss = 0.00815   validation_loss = 0.02361\n",
      "User_id 1442 training:\n",
      "Epoch 999/1000   train_loss = 0.00229   validation_loss = 0.01559\n",
      "User_id 1443 training:\n",
      "Epoch 999/1000   train_loss = 0.00376   validation_loss = 0.00974\n",
      "User_id 1444 training:\n",
      "Epoch 999/1000   train_loss = 0.00336   validation_loss = 0.00772\n",
      "User_id 1445 training:\n",
      "Epoch 999/1000   train_loss = 0.01422   validation_loss = 0.07055\n",
      "User_id 1446 training:\n",
      "Epoch 999/1000   train_loss = 0.00457   validation_loss = 0.03235\n",
      "User_id 1447 training:\n",
      "Epoch 999/1000   train_loss = 0.00573   validation_loss = 0.02117\n",
      "User_id 1448 training:\n",
      "Epoch 999/1000   train_loss = 0.01558   validation_loss = 0.04482\n",
      "User_id 1449 training:\n",
      "Epoch 999/1000   train_loss = 0.00576   validation_loss = 0.02667\n",
      "User_id 1450 training:\n",
      "Epoch 999/1000   train_loss = 0.00453   validation_loss = 0.01617\n",
      "User_id 1451 training:\n",
      "Epoch 999/1000   train_loss = 0.00459   validation_loss = 0.03238\n",
      "User_id 1452 training:\n",
      "Epoch 999/1000   train_loss = 0.01278   validation_loss = 0.09235\n",
      "User_id 1453 training:\n",
      "Epoch 999/1000   train_loss = 0.00346   validation_loss = 0.02344\n",
      "User_id 1454 training:\n",
      "Epoch 999/1000   train_loss = 0.00724   validation_loss = 0.02327\n"
     ]
    }
   ],
   "source": [
    "# Read the prediction set\n",
    "add_data = pd.read_csv(\"prediction/predict.csv\")\n",
    "data_path = \"EN_Tianchi_power_v2.csv\"    \n",
    "\n",
    "\n",
    "for user_id in range(1300, 1455):\n",
    "    # Read the dataset\n",
    "    data = readdata(data_path, user_id)    \n",
    "    # split the dataset into features and targets\n",
    "    features, targets = splitting_data(data)\n",
    "    \n",
    "    # add the text set to the feature set\n",
    "    features = add_predict(features, add_data)\n",
    "    \n",
    "    # dummy the categorical variables\n",
    "    features = dummy_variables(features)\n",
    "    \n",
    "    # Use a dictionary to save the scaling value\n",
    "    scaled_features = {}\n",
    "    \n",
    "    # Using max-min scaleding for features\n",
    "    quant_features = [\"temp\"]\n",
    "    features, scaled_features = scaling_variable(features, scaled_features, quant_features)\n",
    "    \n",
    "    # Using max-min scaleding for targets\n",
    "    quant_features = [\"power_consumption\"]\n",
    "    targets, scaled_features = scaling_variable(targets, scaled_features, quant_features)\n",
    "    \n",
    "    # spliting the test set, validation set and training set\n",
    "    train_features, train_targets, val_features, val_targets, test_features = splitting_validation_test(features, targets)\n",
    "    \n",
    "    # Use a dictionary to save the loss so that we can show as a figure to analysis\n",
    "    losses = {'train':[], 'validation':[]}\n",
    "    \n",
    "    print(\"User_id \" + str(user_id) + \" training:\")\n",
    "    # Training the model for each company\n",
    "    losses, validation, prediction = build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, \n",
    "                                                        train_features, train_targets, val_features, val_targets, test_features, leaky_relu)\n",
    "    # plot the loss\n",
    "    #plot_loss(losses, user_id)\n",
    "    \n",
    "    # plot the validation test\n",
    "    #draw_validation(validation, val_targets, scaled_features, user_id)\n",
    "    \n",
    "    # read the max and min value to change the prediction into a real form\n",
    "    max_, min_ = scaled_features['power_consumption']\n",
    "    \n",
    "    # saving the prediction\n",
    "    saving_answer(prediction*(max_ - min_) + min_, user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
