{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readdata(data_path, user_id):\n",
    "    power = pd.read_csv(data_path)\n",
    "    power = power.loc[(power.user_id == user_id), :]\n",
    "    data = power.drop([\"user_id\"], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting_data(data):\n",
    "    # Separate the data into features and targets\n",
    "    target_fields = ['power_consumption']\n",
    "    features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "\n",
    "    return features, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_predict(feature, pre):\n",
    "    feature = feature.append(pre, ignore_index=True)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummy_variables(data):\n",
    "    dummy_fields = ['year', 'month', 'day', 'weekday', 'season', 'climate', 'windspeed']\n",
    "\n",
    "    for each in dummy_fields:\n",
    "        dummies = pd.get_dummies(data[each], prefix=each, drop_first=False)\n",
    "        data = pd.concat([data, dummies], axis=1)\n",
    "\n",
    "    fields_to_drop = ['year', 'month', 'day', 'weekday', 'season', 'climate', 'windspeed']\n",
    "    data = data.drop(fields_to_drop, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scaling_variable(data, scaled_features, quant_features):\n",
    "    # Store scalings in a dictionary so we can convert back later\n",
    "    for each in quant_features:\n",
    "        max_, min_ = data[each].max(), data[each].min()\n",
    "        scaled_features[each] = [max_, min_]\n",
    "        data.loc[:, each] = (data[each] - min_)/(max_ - min_)\n",
    "    return data, scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitting_validation_test(feature, target):\n",
    "    # splitting test set\n",
    "    train_features, test_features, train_targets = features[:-30], features[-30:], targets\n",
    "    \n",
    "    #splitting validation set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_features, val_features, train_targets, val_targets = train_test_split(train_features, train_targets, test_size=0.1, random_state=42)\n",
    "    \n",
    "    return train_features, train_targets, val_features, val_targets, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def saving_answer(predicton, user_id):\n",
    "    answer = pd.read_csv(\"prediction/answer.csv\")\n",
    "    answer[\"id\"+str(user_id)] = predicton.reshape(len(predicton)).tolist()\n",
    "    answer.to_csv(\"prediction/answer.csv\", index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer.  You are using {}'.format(tf.__version__)\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.2, name='leaky_relu'):\n",
    "    return tf.maximum(x, alpha * x, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(features,targets, batch_size):\n",
    "    batch_number = len(features)//batch_size\n",
    "    for i in range(batch_number+1):\n",
    "        start = i*batch_size\n",
    "        end = start + batch_size\n",
    "        if i == batch_number:\n",
    "            yield features[start:], targets[start:]\n",
    "        else:\n",
    "            yield features[start:end], targets[start:end]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Create_variable():\n",
    "    input_ = tf.placeholder(tf.float32, [None, 65], name = \"inputs\")  # input\n",
    "    label_ = tf.placeholder(tf.float32, [None, 1], name = \"outputs\")  # output \n",
    "    keep_prob = tf.placeholder(tf.float32, name = \"keep_prob\")        # probability to keep units\n",
    "    lr = tf.placeholder(tf.float32, name = \"learning_rate\")           # learning rate\n",
    "    \n",
    "    return input_, label_, keep_prob, lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(ac_fn, keep_prob, input_, label_):\n",
    "    layer1 = tf.layers.dense(input_, 128, activation = ac_fn, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    dropout1 = tf.nn.dropout(layer1, keep_prob)\n",
    "\n",
    "    layer2 = tf.layers.dense(dropout1, 256, activation = leaky_relu, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    dropout2 = tf.nn.dropout(layer2, keep_prob)\n",
    "\n",
    "    logits = tf.layers.dense(dropout2, 1, kernel_initializer = tf.contrib.layers.xavier_initializer())\n",
    "    cost = tf.losses.mean_squared_error(logits, label_)\n",
    "    \n",
    "    return logits, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_opt(cost, learning_rate, beta1 = 0.5):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate, beta1).minimize(cost)\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, train_features, train_targets, \n",
    "                         val_features, val_targets, test_features, ac_fn):\n",
    "\n",
    "    input_, label_, keep_prob, lr = Create_variable()\n",
    "    logits, cost = model_loss(ac_fn, keep_prob, input_, label_)\n",
    "    optimizer = model_opt(cost, learning_rate)\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for epoch_i in range(epoch_count):\n",
    "            step = 0\n",
    "            for x, y in get_batches(train_features.values,train_targets.values, batch_size):\n",
    "                step += 1\n",
    "                feed = {input_: x, label_: y,  keep_prob: dropout, lr: learning_rate}\n",
    "                train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "\n",
    "            \n",
    "            feed={input_: val_features.values, label_: val_targets.values,  keep_prob: 1, lr: learning_rate}        \n",
    "            val_loss = sess.run(cost, feed_dict=feed)\n",
    "            \n",
    "            \n",
    "        print('Epoch {:>3}/{}   train_loss = {:.5f}   validation_loss = {:.5f}'.format(\n",
    "            epoch_i,\n",
    "            epoch_count,\n",
    "            train_loss,\n",
    "            val_loss))\n",
    "            \n",
    "                #losses['train'].append(train_loss)\n",
    "                #losses['validation'].append(val_loss)\n",
    "            \n",
    "        # validation result\n",
    "        feed={input_: val_features.values,  keep_prob: 1, lr: learning_rate}\n",
    "        validation = sess.run(logits, feed_dict=feed)\n",
    "        \n",
    "        # prediction result\n",
    "        feed={input_: test_features.values,  keep_prob: 1, lr: learning_rate}\n",
    "        prediction = sess.run(logits, feed_dict=feed)\n",
    "        \n",
    "        return losses, validation, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epoch_count = 2000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "dropout = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_loss(losses, user_id):\n",
    "    print(\"losses for user_id \" + str(user_id))\n",
    "    \n",
    "    plt.plot(losses['train'], label='Training loss')\n",
    "    plt.plot(losses['validation'], label='Validation loss')\n",
    "    plt.legend()\n",
    "    plt.ylim(ymax=0.02, ymin=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def draw_validation(validation, val_targets, scaled_features, user_id):\n",
    "    print(\"validation test for user_id \" + str(user_id))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    max_, min_ = scaled_features['power_consumption']\n",
    "    pre = validation*(max_ - min_) + min_\n",
    "    ax.plot(pre, label='Prediction')\n",
    "    ax.scatter(range(len(val_targets)), pre)\n",
    "    val = (val_targets*(max_ - min_) + min_).values\n",
    "    ax.plot(val, label='Data')\n",
    "    ax.scatter(range(len(val_targets)), val)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Saving the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_id 1 training:\n",
      "Epoch 1999/2000   train_loss = 0.00207   validation_loss = 0.01017\n",
      "User_id 2 training:\n",
      "Epoch 1999/2000   train_loss = 0.00394   validation_loss = 0.04075\n",
      "User_id 3 training:\n",
      "Epoch 1999/2000   train_loss = 0.00387   validation_loss = 0.02345\n",
      "User_id 4 training:\n",
      "Epoch 1999/2000   train_loss = 0.00436   validation_loss = 0.01239\n",
      "User_id 5 training:\n",
      "Epoch 1999/2000   train_loss = 0.00348   validation_loss = 0.00716\n",
      "User_id 6 training:\n",
      "Epoch 1999/2000   train_loss = 0.00466   validation_loss = 0.02310\n",
      "User_id 7 training:\n",
      "Epoch 1999/2000   train_loss = 0.00383   validation_loss = 0.00746\n",
      "User_id 8 training:\n",
      "Epoch 1999/2000   train_loss = 0.00194   validation_loss = 0.01338\n",
      "User_id 9 training:\n",
      "Epoch 1999/2000   train_loss = 0.00494   validation_loss = 0.02093\n",
      "User_id 10 training:\n",
      "Epoch 1999/2000   train_loss = 0.00611   validation_loss = 0.02188\n",
      "User_id 11 training:\n",
      "Epoch 1999/2000   train_loss = 0.00269   validation_loss = 0.01278\n",
      "User_id 12 training:\n",
      "Epoch 1999/2000   train_loss = 0.00183   validation_loss = 0.00779\n",
      "User_id 13 training:\n",
      "Epoch 1999/2000   train_loss = 0.00446   validation_loss = 0.04903\n",
      "User_id 14 training:\n",
      "Epoch 1999/2000   train_loss = 0.00922   validation_loss = 0.05360\n",
      "User_id 15 training:\n",
      "Epoch 1999/2000   train_loss = 0.00136   validation_loss = 0.00570\n",
      "User_id 16 training:\n",
      "Epoch 1999/2000   train_loss = 0.00111   validation_loss = 0.01243\n",
      "User_id 17 training:\n",
      "Epoch 1999/2000   train_loss = 0.01468   validation_loss = 0.04396\n",
      "User_id 18 training:\n",
      "Epoch 1999/2000   train_loss = 0.00287   validation_loss = 0.00308\n",
      "User_id 19 training:\n",
      "Epoch 1999/2000   train_loss = 0.00401   validation_loss = 0.01266\n",
      "User_id 20 training:\n",
      "Epoch 1999/2000   train_loss = 0.00257   validation_loss = 0.00188\n",
      "User_id 21 training:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0b61ee9cc146>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[1;31m# Training the model for each company\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     losses, validation, prediction = build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, \n\u001b[0;32m---> 38\u001b[0;31m                                                         train_features, train_targets, val_features, val_targets, test_features, leaky_relu)\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[1;31m# plot the loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[1;31m#plot_loss(losses, user_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-4c41fc8020d9>\u001b[0m in \u001b[0;36mbuild_neural_network\u001b[0;34m(losses, epoch_count, batch_size, learning_rate, dropout, train_features, train_targets, val_features, val_targets, test_features, ac_fn)\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mfeed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                 \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\envs\\dlnd-tf-lab\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read the prediction set\n",
    "add_data = pd.read_csv(\"prediction/predict.csv\")\n",
    "data_path = \"EN_Tianchi_power_v2.csv\"    \n",
    "\n",
    "\n",
    "for user_id in range(1, 1455):\n",
    "    # Read the dataset\n",
    "    data = readdata(data_path, user_id)    \n",
    "    # split the dataset into features and targets\n",
    "    features, targets = splitting_data(data)\n",
    "    \n",
    "    # add the text set to the feature set\n",
    "    features = add_predict(features, add_data)\n",
    "    \n",
    "    # dummy the categorical variables\n",
    "    features = dummy_variables(features)\n",
    "    \n",
    "    # Use a dictionary to save the scaling value\n",
    "    scaled_features = {}\n",
    "    \n",
    "    # Using max-min scaleding for features\n",
    "    quant_features = [\"temp\"]\n",
    "    features, scaled_features = scaling_variable(features, scaled_features, quant_features)\n",
    "    \n",
    "    # Using max-min scaleding for targets\n",
    "    quant_features = [\"power_consumption\"]\n",
    "    targets, scaled_features = scaling_variable(targets, scaled_features, quant_features)\n",
    "    \n",
    "    # spliting the test set, validation set and training set\n",
    "    train_features, train_targets, val_features, val_targets, test_features = splitting_validation_test(features, targets)\n",
    "    \n",
    "    # Use a dictionary to save the loss so that we can show as a figure to analysis\n",
    "    losses = {'train':[], 'validation':[]}\n",
    "    \n",
    "    print(\"User_id \" + str(user_id) + \" training:\")\n",
    "    # Training the model for each company\n",
    "    losses, validation, prediction = build_neural_network(losses, epoch_count, batch_size, learning_rate, dropout, \n",
    "                                                        train_features, train_targets, val_features, val_targets, test_features, leaky_relu)\n",
    "    # plot the loss\n",
    "    #plot_loss(losses, user_id)\n",
    "    \n",
    "    # plot the validation test\n",
    "    #draw_validation(validation, val_targets, scaled_features, user_id)\n",
    "    \n",
    "    # read the max and min value to change the prediction into a real form\n",
    "    max_, min_ = scaled_features['power_consumption']\n",
    "    \n",
    "    # saving the prediction\n",
    "    saving_answer(prediction*(max_ - min_) + min_, user_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
